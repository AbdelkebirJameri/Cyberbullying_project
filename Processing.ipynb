{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e735953",
   "metadata": {},
   "source": [
    "# importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65a4061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, random, json, string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import wandb\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import TrainerCallback, AdamW, get_cosine_schedule_with_warmup\n",
    "from transformers import DataCollatorForTokenClassification, PreTrainedModel, RobertaTokenizerFast\n",
    "\n",
    "from datasets import load_dataset, ClassLabel, Sequence, load_metric\n",
    "\n",
    "from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348b3055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: abdel_tracker (abdel_team). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to log in to weights and biases to visualizze our training crosse loss and ephocs\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34be15",
   "metadata": {},
   "source": [
    "## configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff3fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face models roberta \n",
    "models = dict(\n",
    "    ROBERTA = \"roberta-base\",\n",
    "    DISTILBERT_U = \"distilbert-base-uncased\",\n",
    "    DISTILBERT_C = \"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba34f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging date for w&b\"jsut to show data in dandb visualization\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "log_date = today.strftime(\"%d-%m-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cda978eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT='IC-NER'\n",
      "env: WANDB_LOG_MODEL=false\n"
     ]
    }
   ],
   "source": [
    "# LOAD OR TRAIN MODEL\n",
    "TRAIN = 1 # 1 to TRAIN WEIGHTS or 0 to LOAD WEIGHTS\n",
    "\n",
    "# TRAIN/VALIDATION SPLIT\n",
    "TRAIN_SPLIT = 0.90\n",
    "\n",
    "# RANDOM SEED FOR REPRODUCIBILITY\n",
    "RANDOM_SEED = 30\n",
    "\n",
    "# BATCH SIZE\n",
    "\n",
    "BATCH_SIZES = 1\n",
    "\n",
    "# EPOCHS - TRANSFORMERS ARE TYPICALLY FINE-TUNED BETWEEN 1 AND 3 EPOCHS \n",
    "EPOCHS = 3\n",
    "\n",
    "# our model of transformers\n",
    "MODEL_CHECKPOINT = models['ROBERTA']\n",
    "\n",
    "# SPECIFY THE WEIGHTS AND BIASES PROJECT NAME\n",
    "%env WANDB_PROJECT = 'IC-NER' \n",
    "\n",
    "# DETERMINE WHETHER TO SAVE THE MODEL IN THE 100GB OF FREE W&B STORAGE\n",
    "%env WANDB_LOG_MODEL = false "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b6f3d",
   "metadata": {},
   "source": [
    "#  File and dataset handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a9eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########paths\n",
    "FEATURE_CLASS_LABELS = \"feature_class_labels.json\"\n",
    "DATA_FILE = 'v1-annotated.json'\n",
    "TEMP_MODEL_OUTPUT_DIR = 'temp_model_output_dir'\n",
    "SAVED_MODEL = f\"p2d-NER-Fine-Tune-Transformer-{MODEL_CHECKPOINT}\" # Change for notebook version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a3125b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-221eed0ba180e81c\n",
      "Found cached dataset json (C:/Users/ce pc/.cache/huggingface/datasets/json/default-221eed0ba180e81c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c8723abe184dd786cd3926e1b685cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\ce pc\\.cache\\huggingface\\datasets\\json\\default-221eed0ba180e81c\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-80e141d39c0af3f1.arrow and C:\\Users\\ce pc\\.cache\\huggingface\\datasets\\json\\default-221eed0ba180e81c\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-24a78c632a3d4f3c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'ner_tags', 'split_tokens'],\n",
      "        num_rows: 43\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'ner_tags', 'split_tokens'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data_files = DATA_FILE\n",
    "datasets = load_dataset('json', data_files=data_files, field='data')\n",
    "\n",
    "# Create train and validation datasets\n",
    "datasets = datasets['train'].train_test_split(test_size=1-TRAIN_SPLIT, seed=RANDOM_SEED)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0770926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 B-Doxing\n",
      "1 B-Harassment\n",
      "2 B-Insult\n",
      "3 B-Racism\n",
      "4 B-Sexism\n",
      "5 B-Trolling\n",
      "6 I-Doxing\n",
      "7 I-Harassment\n",
      "8 I-Insult\n",
      "9 I-Racism\n",
      "10 I-Sexism\n",
      "11 I-Trolling\n",
      "12 O\n"
     ]
    }
   ],
   "source": [
    "# Opening the label list created in pre-processing corresponding to the ner_tag indices\n",
    "with open(FEATURE_CLASS_LABELS, 'r') as f:\n",
    "    label_list = json.load(f)\n",
    "\n",
    "for n in range(len(label_list)):\n",
    "    print(n, label_list[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07930b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>split_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>[12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 4, 10, 10, 10, 10, 10, 10, 10, 12, 12]</td>\n",
       "      <td>[\", SO, HILARIOUS, U, WRITE, UR, OWN, MATERIAL, ?, @JesseElJefe, A, lot, of, ppl, call, me, sexist, ., But, those, ppl, are, women, ,, and, their, opinions, do, n't, matter, ., \"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>[12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 2, 8, 3, 12, 12, 12, 12, 2, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>[\", @D_Paid, :, @Twin_Thing_Two, :, Y'all, see, this, though, right, !, ?, RT, -------&amp;gt, ;, \", \", @tayyoung, _, :, FUCK, OBAMA, ,, dumb, ass, nigger, \", \", \", \", bitch, Fuck, u\"\"EAT, A, DICK, !, !, \"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>[1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]</td>\n",
       "      <td>[Cardi, talks, like, all, the, girls, in, high, school, that, bullied, me]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking some random samples to ensure data loaded as expected:\n",
    "def show_random_elements(dataset, num_examples=1):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(datasets[\"train\"], num_examples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32c1adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting  the tokenizer\n",
    "\n",
    "\n",
    "if MODEL_CHECKPOINT == models['ROBERTA']:\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e9c4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_id_func(input_ids, print_labs=False):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    word_ids = []\n",
    "    i=0\n",
    "    spec_toks = ['[CLS]', '[SEP]', '[PAD]']\n",
    "    for t in tokens:\n",
    "        if t in spec_toks:\n",
    "            word_ids.append(-100)\n",
    "            print(t, i) if print_labs else None\n",
    "        elif t.startswith('▁'):\n",
    "            i += 1\n",
    "            word_ids.append(i)\n",
    "            print(t, i) if print_labs else None\n",
    "        else:\n",
    "            word_ids.append(i)\n",
    "            print(t, i) if print_labs else None\n",
    "        print(\"Total:\", i) if print_labs else None\n",
    "    return word_ids\n",
    "\n",
    "def tokenize_and_align_labels(examples, label_all_tokens=False):\n",
    "    tokenized_inputs = tokenizer(examples[\"split_tokens\"],\n",
    "                                 truncation=True,\n",
    "                                 is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def tokenize_and_align_labels_deberta(examples, label_all_tokens=False):\n",
    "    tokenized_inputs = tokenizer(examples[\"split_tokens\"],\n",
    "                                 truncation=True,\n",
    "                                 is_split_into_words=True)\n",
    "    labels = []\n",
    "    word_ids_list = []\n",
    "    for input_ids in tokenized_inputs[\"input_ids\"]:\n",
    "        wids = word_id_func(input_ids, print_labs=False)\n",
    "        word_ids_list.append(wids)\n",
    "    \n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = word_ids_list[i]\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx == -100:\n",
    "                label_ids.append(-100)\n",
    "            #We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx-1])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx-1] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "444c4b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02409e2b389f4aba803ee427131cff4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916dee0eb6e14dffac6c2ff39e58f3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#appling the map method\n",
    "#if MODEL_CHECKPOINT == models['DEBERTA_V2_XL'] or MODEL_CHECKPOINT == models['DEBERTA_V2_XXL']:\n",
    "tokenize_and_align_labels = tokenize_and_align_labels_deberta\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True, load_from_cache_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40c275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bdf3652",
   "metadata": {},
   "source": [
    "# Build the  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6207e4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2ee8b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum learning rate is:  7.5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ce pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Optimizer\n",
    "learning_rate = 0.0000075\n",
    "lr_max = learning_rate * BATCH_SIZES\n",
    "weight_decay = 0.05\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=lr_max,\n",
    "    weight_decay=weight_decay)\n",
    "\n",
    "print(\"The maximum learning rate is: \",lr_max)\n",
    "\n",
    "# Learning Rate \n",
    "num_train_samples = len(datasets[\"train\"])\n",
    "warmup_ratio = 0.2 # Percentage of total steps to go from zero to max learning rate\n",
    "num_cycles=0.8 # The cosine exponential rate\n",
    "\n",
    "num_training_steps = num_train_samples*EPOCHS/BATCH_SIZES\n",
    "num_warmup_steps = num_training_steps*warmup_ratio\n",
    "\n",
    "lr_sched = get_cosine_schedule_with_warmup(optimizer=optimizer,\n",
    "                                           num_warmup_steps=num_warmup_steps,\n",
    "                                           num_training_steps = num_training_steps,\n",
    "                                           num_cycles=num_cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f19cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating args\n",
    "args = TrainingArguments(output_dir = TEMP_MODEL_OUTPUT_DIR,\n",
    "                         evaluation_strategy = \"epoch\",\n",
    "                         learning_rate=lr_max,\n",
    "                         per_device_train_batch_size=BATCH_SIZES,\n",
    "                         per_device_eval_batch_size=BATCH_SIZES,\n",
    "                         num_train_epochs=EPOCHS,\n",
    "                         weight_decay=weight_decay,\n",
    "                         lr_scheduler_type = 'cosine',\n",
    "                         warmup_ratio=warmup_ratio,\n",
    "                         logging_strategy=\"epoch\",\n",
    "                         save_strategy=\"epoch\",\n",
    "                         seed=RANDOM_SEED,\n",
    "                         report_to = 'wandb', # enable logging to W&B\n",
    "                         run_name = MODEL_CHECKPOINT+\"-\"+log_date,\n",
    "                         metric_for_best_model=\"f1\",\n",
    "                         load_best_model_at_end = True)   # name of the W&B run (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "420b5d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af0ba17",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab5801ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    # Define the metric parameters\n",
    "    overall_precision = precision_score(true_labels, true_predictions, zero_division=1)\n",
    "    overall_recall = recall_score(true_labels, true_predictions, zero_division=1)\n",
    "    overall_f1 = f1_score(true_labels, true_predictions, zero_division=1)\n",
    "    overall_accuracy = accuracy_score(true_labels, true_predictions)\n",
    "    \n",
    "    # Return a dictionary with the calculated metrics\n",
    "    return {\n",
    "        \"precision\": overall_precision,\n",
    "        \"recall\": overall_recall,\n",
    "        \"f1\": overall_f1,\n",
    "        \"accuracy\": overall_accuracy,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7ea8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining and sreating  the Trainer...\n",
    "trainer = Trainer(\n",
    "                model=model,\n",
    "                args=args,\n",
    "                train_dataset=tokenized_datasets[\"train\"],\n",
    "                eval_dataset=tokenized_datasets[\"test\"],\n",
    "                data_collator=data_collator,\n",
    "                tokenizer=tokenizer,\n",
    "                compute_metrics=compute_metrics,\n",
    "                optimizers=(optimizer, lr_sched)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1ff9e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: ner_tags, split_tokens, id. If ner_tags, split_tokens, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 43\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 129\n",
      "  Number of trainable parameters = 124065037\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93966464c30d4bbfbd1197107a2f1ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01720000000004802, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\ce pc\\Desktop\\testing3\\wandb\\run-20221220_205411-3760znk2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/abdel_team/%27IC-NER%27/runs/3760znk2\" target=\"_blank\">roberta-base-20-12-2022</a></strong> to <a href=\"https://wandb.ai/abdel_team/%27IC-NER%27\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [129/129 15:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.789300</td>\n",
       "      <td>1.895218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.596500</td>\n",
       "      <td>1.048762</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.256800</td>\n",
       "      <td>1.365016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: ner_tags, split_tokens, id. If ner_tags, split_tokens, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to temp_model_output_dir\\checkpoint-43\n",
      "Configuration saved in temp_model_output_dir\\checkpoint-43\\config.json\n",
      "Model weights saved in temp_model_output_dir\\checkpoint-43\\pytorch_model.bin\n",
      "tokenizer config file saved in temp_model_output_dir\\checkpoint-43\\tokenizer_config.json\n",
      "Special tokens file saved in temp_model_output_dir\\checkpoint-43\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: ner_tags, split_tokens, id. If ner_tags, split_tokens, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to temp_model_output_dir\\checkpoint-86\n",
      "Configuration saved in temp_model_output_dir\\checkpoint-86\\config.json\n",
      "Model weights saved in temp_model_output_dir\\checkpoint-86\\pytorch_model.bin\n",
      "tokenizer config file saved in temp_model_output_dir\\checkpoint-86\\tokenizer_config.json\n",
      "Special tokens file saved in temp_model_output_dir\\checkpoint-86\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: ner_tags, split_tokens, id. If ner_tags, split_tokens, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to temp_model_output_dir\\checkpoint-129\n",
      "Configuration saved in temp_model_output_dir\\checkpoint-129\\config.json\n",
      "Model weights saved in temp_model_output_dir\\checkpoint-129\\pytorch_model.bin\n",
      "tokenizer config file saved in temp_model_output_dir\\checkpoint-129\\tokenizer_config.json\n",
      "Special tokens file saved in temp_model_output_dir\\checkpoint-129\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp_model_output_dir\\checkpoint-43 (score: 0.0).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=129, training_loss=1.2142170824745828, metrics={'train_runtime': 930.5877, 'train_samples_per_second': 0.139, 'train_steps_per_second': 0.139, 'total_flos': 3125154597786.0, 'train_loss': 1.2142170824745828, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c765a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: ner_tags, split_tokens, id. If ner_tags, split_tokens, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7428363561630249,\n",
       " 'eval_precision': 1.0,\n",
       " 'eval_recall': 0.0,\n",
       " 'eval_f1': 0.0,\n",
       " 'eval_accuracy': 0.896774193548387,\n",
       " 'eval_runtime': 2.761,\n",
       " 'eval_samples_per_second': 1.811,\n",
       " 'eval_steps_per_second': 1.811,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate based on the chosen epoch (usually best or last)\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a896c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▁▁</td></tr><tr><td>eval/f1</td><td>▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▄▁█</td></tr><tr><td>eval/precision</td><td>▁▁▁▁</td></tr><tr><td>eval/recall</td><td>▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▁▅▃</td></tr><tr><td>eval/samples_per_second</td><td>▁█▃▆</td></tr><tr><td>eval/steps_per_second</td><td>▁█▃▆</td></tr><tr><td>train/epoch</td><td>▁▁▅▅████</td></tr><tr><td>train/global_step</td><td>▁▁▅▅████</td></tr><tr><td>train/learning_rate</td><td>█▁▆</td></tr><tr><td>train/loss</td><td>█▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.89677</td></tr><tr><td>eval/f1</td><td>0.0</td></tr><tr><td>eval/loss</td><td>0.74284</td></tr><tr><td>eval/precision</td><td>1.0</td></tr><tr><td>eval/recall</td><td>0.0</td></tr><tr><td>eval/runtime</td><td>2.761</td></tr><tr><td>eval/samples_per_second</td><td>1.811</td></tr><tr><td>eval/steps_per_second</td><td>1.811</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>129</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.7122</td></tr><tr><td>train/total_flos</td><td>3125154597786.0</td></tr><tr><td>train/train_loss</td><td>1.02643</td></tr><tr><td>train/train_runtime</td><td>826.7436</td></tr><tr><td>train/train_samples_per_second</td><td>0.156</td></tr><tr><td>train/train_steps_per_second</td><td>0.156</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">roberta-base-20-12-2022</strong>: <a href=\"https://wandb.ai/abdel_team/%27IC-NER%27/runs/tgvbqz7i\" target=\"_blank\">https://wandb.ai/abdel_team/%27IC-NER%27/runs/tgvbqz7i</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221220_201130-tgvbqz7i\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finish Weighs & Biases logging for this run\n",
    "wandb.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "796f965e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p2d-NER-Fine-Tune-Transformer-roberta-base\n",
      "Configuration saved in p2d-NER-Fine-Tune-Transformer-roberta-base\\config.json\n",
      "Model weights saved in p2d-NER-Fine-Tune-Transformer-roberta-base\\pytorch_model.bin\n",
      "tokenizer config file saved in p2d-NER-Fine-Tune-Transformer-roberta-base\\tokenizer_config.json\n",
      "Special tokens file saved in p2d-NER-Fine-Tune-Transformer-roberta-base\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340de108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7224e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74f9ff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file p2d-NER-Fine-Tune-Transformer-roberta-base\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"p2d-NER-Fine-Tune-Transformer-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file p2d-NER-Fine-Tune-Transformer-roberta-base\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "All the weights of RobertaForTokenClassification were initialized from the model checkpoint at p2d-NER-Fine-Tune-Transformer-roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = AutoModelForTokenClassification.from_pretrained(SAVED_MODEL)\n",
    "\n",
    "pred_trainer = Trainer(\n",
    "    loaded_model,\n",
    "    args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e935ccb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: ner_tags, split_tokens, id. If ner_tags, split_tokens, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Insult       1.00      0.00      0.00        13\n",
      "      Racism       1.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       1.00      0.00      0.00        15\n",
      "   macro avg       1.00      0.00      0.00        15\n",
      "weighted avg       1.00      0.00      0.00        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the predictions and produce a classification report\n",
    "predictions, labels, _ = pred_trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "# Generate the metrics and display\n",
    "results = classification_report(true_labels, true_predictions, zero_division=1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb8ec596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "27\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "check = 3\n",
    "\n",
    "print(len(datasets[\"test\"][check]['split_tokens']))\n",
    "print(len(true_predictions[check]))\n",
    "print(len(true_labels[check]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed332380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the predicted extracted data\n",
    "check_pred = zip(datasets[\"test\"][check]['split_tokens'], true_predictions[check])\n",
    "for tup in check_pred:\n",
    "    if tup[1] != 'O':\n",
    "        print(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3afce19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Muthafuckas', 'B-Insult')\n",
      "('fuck', 'B-Insult')\n",
      "('dumb', 'B-Insult')\n",
      "('ass', 'I-Insult')\n",
      "('niggers', 'B-Racism')\n"
     ]
    }
   ],
   "source": [
    "# Compare to the actual labels\n",
    "check_true = zip(datasets[\"test\"][check]['split_tokens'], true_labels[check])\n",
    "for tup in check_true:\n",
    "    if tup[1] != 'O':\n",
    "        print(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e7aadbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (manager-core).\n",
      "Your token has been saved to C:\\Users\\ce pc\\.huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eba3a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git init && git remote add origin && git pull origin main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b68a777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f50e443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
